{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "487905c5",
   "metadata": {},
   "source": [
    "<h2>TWEET CLASSIFICATION MODEL (BASIC) </h2>\n",
    "\n",
    "The following Jupyter notebook contains implementation for a basic classification model for classifying tweets \n",
    "into \"Positive\", \"Negative\", \"Nuetral\" cateogories.\n",
    "This has been implemented using the following classifiers:\n",
    "1) RandomForest<br>\n",
    "2) LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf0ec9",
   "metadata": {},
   "source": [
    "<h3>1. DATA PREPROCESSING</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e38eaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet                                  4104\n",
       "twitterHandlerused                     4104\n",
       "label                                  4104\n",
       "fasttext_labels                        4104\n",
       "Comparison_human&ft                      87\n",
       "Labels_Manish_Krishna_Anu_Harvinder       2\n",
       "2nd review                                0\n",
       "Final_labels                           4104\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Downloaded csv file for training data and creating a dataframe for pre-processing\n",
    "data = pd.read_csv('Aitrainingdata.csv')\n",
    "#Preprocessing of data , checking for null values \n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd43d9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @HatePatroller All @mindvalley students are li...\n",
       "1    RT @AlphaGammaHQ: Conferences are great platfo...\n",
       "2    @AlphaGammaHQ @wobi_en @GIFLondon @Esportsbzsu...\n",
       "3    RT @mindvalley: You asked, we delivered. ÃƒÂ°Ã...\n",
       "4    @dubeji18 Check this one out by @thesleepdocto...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweet without cleaning \n",
    "data['tweet'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb353ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import preprocessor as p\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b6344",
   "metadata": {},
   "source": [
    "<h4> 1.1 Tokenizing, Stopword and Punctuation Removal</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b2c93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to apply cleaning to all tweets by removing usernames, special characters and filtering out the tweet body\n",
    "def clean_tweet(row):\n",
    "    '''  \n",
    "    Parameters\n",
    "    ----------\n",
    "    row : dataframe row        \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tweet : df column values\n",
    "    cleaned tweet from df row  \n",
    "\n",
    "    '''\n",
    "    tweet = row['tweet']\n",
    "    tweet = p.clean(tweet)\n",
    "    return tweet\n",
    "#Applying to all rows of the tweet column in the data\n",
    "data['tweet'] = data.apply(clean_tweet,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc87f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_rem_stopwords(row):\n",
    "    '''  \n",
    "    Parameters\n",
    "    ----------\n",
    "    row : dataframe row        \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tweet : df column values\n",
    "    stopwords and punctuation removed tweet from df row  \n",
    "\n",
    "    '''\n",
    "    tweet = row['tweet']\n",
    "    tweet = TweetTokenizer().tokenize(tweet)\n",
    "    stop = stopwords.words('english') #Importing stopwords from english\n",
    "    newtweet = []\n",
    "    for ww in tweet:\n",
    "        if ww.lower() not in stop and ww not in string.punctuation:\n",
    "            newtweet.append(ww)\n",
    "    return newtweet\n",
    "#\n",
    "data['tweet'] = data.apply(tokenize_rem_stopwords,axis = 1)\n",
    "data['tweet_token'] = data['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611969c4",
   "metadata": {},
   "source": [
    "<h4> 1.2 Label Encoding </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c886b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "#Label encoding for converting the label into 0,1,2\n",
    "label_encode = LabelEncoder()\n",
    "data['Final_labels'] = label_encode.fit_transform(data['Final_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24d2f727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Conferences great platforms exchange ideas meet like-minded people build Here's\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = data['tweet'][1]\n",
    "list1 = ' '.join(list1)\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef368a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           students like\n",
       "1       Conferences great platforms exchange ideas mee...\n",
       "2                  Awesome list Humble thank including us\n",
       "3       asked delivered years youve asking us make FRE...\n",
       "4       Check one recommended takes consideration natu...\n",
       "                              ...                        \n",
       "4099    Hopefully eventually come use meantime belittl...\n",
       "4100                                          thanks mine\n",
       "4101    hey got text real HSBC ALERT authorised paymen...\n",
       "4102                                          basic bitch\n",
       "4103    Thank letting us know definitely scam ever get...\n",
       "Name: tweet, Length: 4104, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting the tokenized tweet back to string for vectorization\n",
    "def stringyfy(row):\n",
    "    '''  \n",
    "    Parameters\n",
    "    ----------\n",
    "    row : dataframe row        \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tweet : df column values\n",
    "    string version of tokenized sentence\n",
    "    \n",
    "    '''\n",
    "    tweet = row['tweet']\n",
    "    tweet = ' '.join(tweet)\n",
    "    return tweet\n",
    "data['tweet'] = data.apply(stringyfy,axis=1)\n",
    "data['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cfa961",
   "metadata": {},
   "source": [
    "<h4> 1.4 TF-IDF Vectorization </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af90ba3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4104, 7209)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a bag of sentences of all tweets for vectorizing\n",
    "bagOfSentences = data['tweet'].to_list()\n",
    "#for converting the string to vector(numeric) using countvectorizer and tfidf transform\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "cv = CountVectorizer()\n",
    "word_count = cv.fit_transform(bagOfSentences)\n",
    "word_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53cff07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4104, 7209)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count)\n",
    "tf_idf_vector=tfidf_transformer.transform(word_count).toarray()\n",
    "#Tweets in idf vector format\n",
    "X = tf_idf_vector\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee256e1",
   "metadata": {},
   "source": [
    "<h3> 2. CLASSIFICATION USING RANDOM FOREST </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01d1e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = data.loc[:,'tweet':'fasttext_labels']\n",
    "#Splitting of data into train and test set with 80:20 ratio\n",
    "Y = data['Final_labels']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,random_state=1403,train_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7675b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification using RandomForest on the tf-idf vector\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "rf_clf = RandomForestClassifier(n_estimators=250, random_state=0) \n",
    "rf_clf.fit(X_train, y_train) \n",
    "y_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e6f5293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on test data using random classifier 74.72594397076736\n"
     ]
    }
   ],
   "source": [
    "#Accuracy on the test data \n",
    "print(\"The accuracy on test data using random classifier\",np.mean(y_pred==y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c69b00",
   "metadata": {},
   "source": [
    "<h3> 3. CLASSIFICATION USING TENSORFLOW MODEL </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b529a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import for tensorflow and Keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU,SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D, Dense, Activation, Dropout, Input\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "#Keras uses tokenized  data for model training\n",
    "X_deep = data['tweet_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0f9d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize words using tokenizer and max word length to 600\n",
    "max_len = 600\n",
    "tok = Tokenizer(num_words=2000)\n",
    "tok.fit_on_texts(X_deep)\n",
    "sequences = tok.texts_to_sequences(X_deep)\n",
    "#Creating sequence matrix to represent each tweet in a vector format\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "X_train_deep, X_test_deep, y_train_deep, y_test_deep = train_test_split(sequences_matrix,Y,random_state=1403,train_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "192280f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_based_model(): #Defined tensorflow_based_model function for training tenforflow based model with 6 layers\n",
    "    inputs = Input(name='inputs',shape=[max_len])#step1\n",
    "    layer = Embedding(2000,50,input_length=max_len)(inputs) #step2\n",
    "    layer = LSTM(64)(layer) #step3\n",
    "    layer = Dense(256,name='FC1')(layer) #step4\n",
    "    layer = Activation('relu')(layer) # step5\n",
    "    layer = Dropout(0.5)(layer) # step6\n",
    "    layer = Dense(1,name='out_layer')(layer) #step4 \n",
    "    layer = Activation('sigmoid')(layer) #step5 \n",
    "    model = Model(inputs=inputs,outputs=layer) \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d67db70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "10/10 [==============================] - 12s 929ms/step - loss: 0.1993 - accuracy: 0.6179 - val_loss: -2.7965 - val_accuracy: 0.6463\n",
      "Epoch 2/6\n",
      "10/10 [==============================] - 13s 1s/step - loss: -1.5506 - accuracy: 0.6707 - val_loss: -6.9565 - val_accuracy: 0.6463\n",
      "Epoch 3/6\n",
      "10/10 [==============================] - 10s 1s/step - loss: -3.0107 - accuracy: 0.6707 - val_loss: -10.5936 - val_accuracy: 0.6463\n",
      "Epoch 4/6\n",
      "10/10 [==============================] - 11s 1s/step - loss: -4.1042 - accuracy: 0.6707 - val_loss: -13.1540 - val_accuracy: 0.6463\n",
      "Epoch 5/6\n",
      "10/10 [==============================] - 12s 1s/step - loss: -5.1445 - accuracy: 0.6707 - val_loss: -16.6786 - val_accuracy: 0.6463\n",
      "Epoch 6/6\n",
      "10/10 [==============================] - 13s 1s/step - loss: -6.1473 - accuracy: 0.6707 - val_loss: -19.0213 - val_accuracy: 0.6463\n",
      "Training finished !!\n"
     ]
    }
   ],
   "source": [
    "model = tensorflow_based_model()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "history=model.fit(X_train_deep,y_train_deep,batch_size=80,epochs=6, validation_split=0.1)# here we are starting the training of model by feeding the training data\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "025e8fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 6s 63ms/step - loss: -7.7838 - accuracy: 0.6529\n"
     ]
    }
   ],
   "source": [
    "#Accuracy of the model on the test data\n",
    "accr1 = model.evaluate(X_test_deep,y_test_deep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vevn_nlp",
   "language": "python",
   "name": "vevn_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
